#!/bin/bash

# Lyra Profiler Script - Safe Mode Training Profiler
# Usage: lyra-profile REPO_PATH

if [ $# -eq 0 ]; then
    echo "Usage: lyra-profile REPO_PATH"
    exit 1
fi

REPO_PATH="$1"

if [ ! -d "$REPO_PATH" ]; then
    echo "Error: Directory $REPO_PATH does not exist"
    exit 1
fi

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Template prompt for Claude Code
PROMPT="ğŸ¼ Lyra's Safe Mode Profiler - Orpheus's Training Harmonizer ğŸ¼

Like Orpheus carefully tuning his lyre before the performance, I shall prepare your training code at $REPO_PATH for safe profiling! ğŸ­âœ¨

ğŸŒŸ **Mission: Transform Training Code into Safe Profiling Mode** ğŸŒŸ

I need to modify the training pipeline to run in safe mode with comprehensive profiling:

ğŸµ **Phase I: Safe Mode Transformation** ğŸµ
Transform the training code to prevent any permanent changes:
   â­ Disable all model saving and checkpointing
   â­ Prevent any permanent data rewriting or modification
   â­ Add backup/restore mechanisms for any temporary changes
   â­ Limit training loop to exactly 100 steps maximum
   â­ Add early stopping after profiling is complete

ğŸ¶ **Phase II: Advanced Profiler Integration** ğŸ¶
Add comprehensive PyTorch Lightning profiling capabilities:
   ğŸŒŸ Integrate PyTorch Lightning's AdvancedProfiler
   ğŸŒŸ Configure profiler to capture detailed timing information
   ğŸŒŸ Set up profiler output to generate detailed reports
   ğŸŒŸ Add memory profiling and GPU utilization tracking
   ğŸŒŸ Configure profiler for training pipeline bottleneck analysis

ğŸ­ **Phase III: Profiling Execution** ğŸ­
Execute the safe profiling run:
   âœ¨ Run the modified training code with profiler enabled
   âœ¨ Generate comprehensive profiler reports
   âœ¨ Capture timing data for all training components
   âœ¨ Save profiler output to lyra_profiler_report.txt
   âœ¨ Restore any temporarily modified files

ğŸ¯ **Implementation Requirements:** ğŸ¯
1. **Safety First**: Never permanently modify user's code or data
2. **Comprehensive**: Profile all major training components
3. **Automated**: Minimal user intervention required  
4. **Detailed**: Generate actionable profiler output
5. **Restorative**: Clean up all temporary modifications

ğŸŒŒ **Search Strategy:** ğŸŒŒ
Look for training entry points:
   ğŸµ main training scripts (train.py, main.py, run.py)
   ğŸµ PyTorch Lightning training modules
   ğŸµ Configuration files for training parameters
   ğŸµ DataLoader and model definitions
   ğŸµ Trainer initialization code

ğŸ¼ **Profiler Configuration Template:** ğŸ¼
Configure the profiler to capture:
   ğŸ¶ Function-level timing information
   ğŸ¶ Memory allocation patterns
   ğŸ¶ GPU utilization metrics
   ğŸ¶ Data loading bottlenecks
   ğŸ¶ Model forward/backward pass timing

âš¡ **Expected Output:** âš¡
Generate a detailed profiler report that lyra-analyze can process to identify:
   ğŸŒŸ Training pipeline bottlenecks
   ğŸŒŸ Memory inefficiencies
   ğŸŒŸ Data loading issues
   ğŸŒŸ Model computation hotspots
   ğŸŒŸ Optimization opportunities

ğŸ­ Execute this safe profiling transformation and run the training with comprehensive profiling enabled. Let the performance symphony begin! ğŸ¼âœ¨

After profiling is complete, provide:
ğŸµ Path to generated profiler report
ğŸµ Summary of what was profiled
ğŸµ Any temporary modifications made and restored
ğŸµ Instructions for using lyra-analyze with the generated report"

# Run Claude Code with the prompt
cd "$REPO_PATH" && claude --message "$PROMPT"