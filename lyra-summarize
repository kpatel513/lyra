#!/bin/bash

# Lyra Repository Analysis Script
# Usage: lyra-summarize REPO_PATH

if [ $# -eq 0 ]; then
    echo "Usage: lyra-summarize REPO_PATH"
    exit 1
fi

REPO_PATH="$1"

if [ ! -d "$REPO_PATH" ]; then
    echo "Error: Directory $REPO_PATH does not exist"
    exit 1
fi

# Template prompt for Claude Code
PROMPT="ğŸµ Welcome to Lyra - Your AI-powered repository harmonizer! ğŸµ

Like Orpheus with his lyre, I shall weave through your codebase to create a melodious analysis of $REPO_PATH âœ¨

ğŸ¼ **Analysis Symphony - Movement I: Mixed Precision Training** ğŸ¼
Search the constellation of code for evidence of mixed precision training usage, including:
   â­ Automatic Mixed Precision (AMP) implementations
   â­ FP16/BF16 data types
   â­ GradScaler usage
   â­ Mixed precision configuration files
   â­ Training scripts with precision settings

ğŸ¶ **Analysis Symphony - Movement II: Sharding & Distribution** ğŸ¶
Discover the harmonic patterns of distributed training and model/data sharding:
   ğŸŒŸ Data parallel training setups
   ğŸŒŸ Model parallelism configurations
   ğŸŒŸ Tensor sharding strategies
   ğŸŒŸ Distributed training frameworks (DeepSpeed, FairScale, etc.)
   ğŸŒŸ Pipeline parallelism implementations

ğŸ¯ For each celestial finding, please provide:
ğŸµ File locations with line numbers
ğŸµ Brief code snippets showing the implementation
ğŸµ Configuration details if available
ğŸµ Framework/library used (PyTorch, TensorFlow, JAX, etc.)

ğŸ­ Generate a harmonious report with clear YES/NO answers for both categories, followed by supporting evidence and implementation details. Let the music of code analysis begin! ğŸ¼âœ¨"

# Run Claude Code with the prompt
cd "$REPO_PATH" && claude --message "$PROMPT"