#!/bin/bash

# Lyra Repository Analysis Script
# Usage: lyra-summarize REPO_PATH

if [ $# -eq 0 ]; then
    echo "Usage: lyra-summarize REPO_PATH"
    exit 1
fi

REPO_PATH="$1"

# Validate repository path
if [ ! -d "$REPO_PATH" ]; then
    echo "âŒ Error: Directory $REPO_PATH does not exist"
    exit 1
fi

if [ ! -r "$REPO_PATH" ]; then
    echo "âŒ Error: Directory $REPO_PATH is not readable"
    exit 1
fi

# Convert to absolute path
REPO_PATH="$(cd "$REPO_PATH" && pwd)"

# Lyra will use a temporary file for the prompt to avoid shell escaping issues

# Display progress information
echo "ðŸŽ¼ Lyra Repository Analysis Starting..."
echo "ðŸ“‚ Repository: $REPO_PATH"
echo ""
echo "ðŸŽµ Initializing Lyra's harmonic analysis..."
echo "â­ Searching for mixed precision training implementations..."
echo "ðŸŒŸ Analyzing distributed training and sharding patterns..."
echo "ðŸŽ¶ Weaving together the musical threads of your codebase..."
echo ""
echo "ðŸŽ­ Please wait while Orpheus performs his code analysis symphony..."
echo "   This may take 1-3 minutes depending on repository size..."
echo ""

# Run Claude Code with the prompt using a temporary file to avoid shell escaping issues
TEMP_PROMPT_FILE=$(mktemp)
cat > "$TEMP_PROMPT_FILE" << 'EOF'
ðŸŽµ Welcome to Lyra - Your AI-powered repository harmonizer! ðŸŽµ

Like Orpheus with his lyre, I shall weave through your codebase to create a melodious analysis âœ¨

ðŸŽ¼ **Analysis Symphony - Movement I: Mixed Precision Training** ðŸŽ¼
Search the constellation of code for evidence of mixed precision training usage, including:
   â­ Automatic Mixed Precision (AMP) implementations
   â­ FP16/BF16 data types
   â­ GradScaler usage
   â­ Mixed precision configuration files
   â­ Training scripts with precision settings

ðŸŽ¶ **Analysis Symphony - Movement II: Sharding & Distribution** ðŸŽ¶
Discover the harmonic patterns of distributed training and model/data sharding:
   ðŸŒŸ Data parallel training setups
   ðŸŒŸ Model parallelism configurations
   ðŸŒŸ Tensor sharding strategies
   ðŸŒŸ Distributed training frameworks (DeepSpeed, FairScale, etc.)
   ðŸŒŸ Pipeline parallelism implementations

ðŸŽ¼ For each celestial finding, please provide:
ðŸŽµ File locations with line numbers
ðŸŽµ Brief code snippets showing the implementation
ðŸŽµ Configuration details if available
ðŸŽµ Framework/library used (PyTorch, TensorFlow, JAX, etc.)

ðŸŽ­ Generate a harmonious report with clear YES/NO answers for both categories, followed by supporting evidence and implementation details. Let the music of code analysis begin! ðŸŽ¼âœ¨
EOF

if cd "$REPO_PATH" && claude --print "$(cat "$TEMP_PROMPT_FILE")"; then
    rm -f "$TEMP_PROMPT_FILE"
    # Display completion message
    echo ""
    echo "ðŸŽ¼âœ¨ Analysis complete! The harmonious report has been generated above."
else
    rm -f "$TEMP_PROMPT_FILE"
    echo ""
    echo "âŒ Error: Claude Code analysis failed. This could be due to:"
    echo "   â€¢ Claude Code authentication issues"
    echo "   â€¢ Network connectivity problems"
    echo "   â€¢ Repository size or complexity"
    echo "   â€¢ Insufficient permissions"
    echo ""
    echo "ðŸ”§ Troubleshooting suggestions:"
    echo "   1. Check Claude Code authentication: claude config"
    echo "   2. Try with a smaller test repository"
    echo "   3. Ensure stable internet connection"
    echo "   4. Check repository permissions"
    exit 1
fi